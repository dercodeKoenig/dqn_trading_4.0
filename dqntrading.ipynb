{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265f373a-c8dc-40c3-b8c4-9bf21583a33d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de51e6d-e6fb-4892-aff8-f37a9a8fb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import ta\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb014c-ab4a-41fe-bb23-d474537e1b0a",
   "metadata": {},
   "source": [
    "# Convert candlesticks to scaled inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa8806-38a2-4071-bfcc-7b18c3e007ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_x(sample):\n",
    "        \n",
    "                current_close = sample[-1].c\n",
    "\n",
    "                prev_close = [candle.c for candle in sample]\n",
    "                prev_high = [candle.h for candle in sample]\n",
    "                prev_low = [candle.l for candle in sample]\n",
    "\n",
    "                prev_sma21 = [candle.sma21 for candle in sample]\n",
    "                #prev_sma50 = [candle.sma50 for candle in sample]\n",
    "                prev_sma200 = [candle.sma200 for candle in sample]\n",
    "                \n",
    "                #dl = [candle.dl for candle in sample]\n",
    "                #dh = [candle.dh for candle in sample]\n",
    "\n",
    "                #dl_relative = [-(current_close - dl[o]) / dl[o] for o in range(seq_len)]\n",
    "                #dh_relative = [-(current_close - dh[o]) / dh[o] for o in range(seq_len)]\n",
    "                \n",
    "                prev_sma21_relative = [-(current_close - prev_sma21[o]) / prev_sma21[o] for o in range(seq_len)]\n",
    "                #prev_sma50_relative = [-(current_close - prev_sma50[o]) / prev_sma50[o] for o in range(seq_len)]\n",
    "                prev_sma200_relative = [-(current_close - prev_sma200[o]) / prev_sma200[o] for o in range(seq_len)]\n",
    "\n",
    "                prev_close_relative = [-(current_close - prev_close[o]) / prev_close[o] for o in range(seq_len)]\n",
    "                prev_high_relative = [-(current_close - prev_high[o]) / prev_high[o] for o in range(seq_len)]\n",
    "                prev_low_relative = [-(current_close - prev_low[o]) / prev_low[o] for o in range(seq_len)]\n",
    "\n",
    "                \n",
    "                #scale = 1 / (sample[-1].atr_value / sample[-1].c)\n",
    "                \n",
    "                scale = 1000 # scale price: 0.1% -> 1\n",
    "\n",
    "                prev_sma21_relative_scaled = [i * scale for i in prev_sma21_relative]\n",
    "                #prev_sma50_relative_scaled = [i * scale for i in prev_sma50_relative]\n",
    "                prev_sma200_relative_scaled = [i * scale for i in prev_sma200_relative]\n",
    "\n",
    "                prev_close_relative_scaled = [i * scale for i in prev_close_relative]\n",
    "                prev_low_relative_scaled = [i * scale for i in prev_low_relative]\n",
    "                prev_high_relative_scaled = [i * scale for i in prev_high_relative]\n",
    "\n",
    "                #dl_rel_scaled = [i * scale for i in dl_relative]\n",
    "                #dh_rel_scaled = [i * scale for i in dh_relative]\n",
    "                \n",
    "                prev_rsi_14 = [candle.rsi14 for candle in sample]\n",
    "\n",
    "\n",
    "                x = []\n",
    "                for o in range(len(prev_close)):\n",
    "                    ts = []\n",
    "                    ts.append(prev_close_relative_scaled[o])\n",
    "                    ts.append(prev_high_relative_scaled[o])\n",
    "                    ts.append(prev_low_relative_scaled[o])\n",
    "\n",
    "                    ts.append(prev_sma21_relative_scaled[o])\n",
    "                    #ts.append(prev_sma50_relative_scaled[o])\n",
    "                    ts.append(prev_sma200_relative_scaled[o])\n",
    "                    \n",
    "                    #ts.append(dh_rel_scaled[o])\n",
    "                    #ts.append(dl_rel_scaled[o])\n",
    "                    \n",
    "                    ts.append(prev_rsi_14[o])\n",
    "\n",
    "                    x.append(ts)\n",
    "\n",
    "                x = np.array(x)\n",
    "                return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961c8cd-ca2c-454b-821d-583612be1d50",
   "metadata": {},
   "source": [
    "# The environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2c4a8-bac7-4dfb-892e-d0f759efd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load(file):\n",
    "    f = open(file, \"rb\")\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5201b-2f08-40db-a547-03491cf551c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = int(4*24*2)\n",
    "comission = 25/100000\n",
    "log_interval = 4*24 # environment logs daily returns\n",
    "\n",
    "\n",
    "class candle_class:\n",
    "    pass\n",
    "  \n",
    "order_value = 1000\n",
    "idp = 0.0001/1000 # negative return/candle for no open position\n",
    "\n",
    "class environment():\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def _next_observation(self):\n",
    "            candles = self.candles[self.current_step - seq_len + 1:self.current_step + 1]\n",
    "            \n",
    "            inference_data = sample_to_x(candles)\n",
    "            \n",
    "            return inference_data, np.array([self.position, self.current_win])\n",
    "\n",
    "  \n",
    "  def reset(self, first_reset = False):\n",
    "    self.candles = None\n",
    "    candles_files = os.listdir(\"candles\")\n",
    "    use_file = \"candles/\"+random.choice(candles_files)\n",
    "    #print(use_file)\n",
    "    self.candles = Load(use_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.current_step = 200+seq_len if first_reset == False else random.randint(200+seq_len,len(self.candles) - 1000)\n",
    "    self.position = 0\n",
    "    self.entry_price = 0\n",
    "    self.win = 0\n",
    "    self.current_win = 0\n",
    "    self.startindex = self.current_step\n",
    "    self.last_reward = 0\n",
    "    self.reward_since_last_log = 0\n",
    "\n",
    "    return self._next_observation()\n",
    "\n",
    "  \n",
    "  def close(self):\n",
    "        self.win -= comission * order_value / 2\n",
    "        self.position = 0\n",
    "        self.win+=self.current_win\n",
    "        self.current_win = 0\n",
    "        \n",
    "        \n",
    "  def entry(self):\n",
    "        self.entry_price = self.candles[self.current_step].c\n",
    "        self.win -= comission * order_value / 2\n",
    "\n",
    "  def step(self, action):\n",
    "    \n",
    "    if action == 0:\n",
    "        self.win-=idp*order_value\n",
    "        if self.position != 0:\n",
    "            self.close()\n",
    "    \n",
    "    if action == 1:\n",
    "      #short\n",
    "      if self.position == 1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == -1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = -1\n",
    "        self.entry()\n",
    "        \n",
    "    if action == 2:\n",
    "      #long\n",
    "      if self.position == -1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == 1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = 1\n",
    "        self.entry()\n",
    "        \n",
    "    self.current_step += 1\n",
    "    if self.position != 0:\n",
    "      current_price = self.candles[self.current_step].c\n",
    "      entry = self.entry_price\n",
    "      diff = (current_price - entry) / entry * order_value\n",
    "\n",
    "      if self.position == 1:\n",
    "        self.current_win = diff\n",
    "      if self.position == -1:\n",
    "        self.current_win = -diff\n",
    "\n",
    "    reward_raw = self.win + self.current_win\n",
    "    reward = reward_raw - self.last_reward\n",
    "    self.last_reward = reward_raw\n",
    "    \n",
    "    done = self.current_step == len(self.candles) -1\n",
    "    \n",
    "    if (self.current_step - self.startindex) % log_interval == 0:\n",
    "        log_reward = reward_raw - self.reward_since_last_log\n",
    "        self.reward_since_last_log = reward_raw \n",
    "        file2 = open(\"logs/r2_log.txt\", \"a\")  \n",
    "        file2.write(str(log_reward))\n",
    "        file2.write(\"\\n\")\n",
    "        file2.close()\n",
    "        \n",
    "    \n",
    "    obs = self._next_observation()\n",
    "    return obs, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2c027-8379-4dbb-9d61-4cfe76b0b456",
   "metadata": {},
   "source": [
    "# The agent class\n",
    "Agent ist specialized for only this environment and not gym-compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2c10a-60f9-41f8-a1a5-cb93546da34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 10000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                ):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "   \n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "        \n",
    " \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name, overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = True)\n",
    "    def model_call(self, x):\n",
    "        return tf.math.argmax(self.model(x), axis = 1)\n",
    "    \n",
    "    def select_actions(self, current_states, positions):\n",
    "        return self.model_call([current_states, positions]).numpy()\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state])\n",
    "        \n",
    "    @tf.function(jit_compile = True)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "    \n",
    "        \n",
    "    @tf.function(jit_compile = False) # jit not working for whatever reason\n",
    "    def tstep(self, states, next_states, rewards, terminals, masks):\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            estimated_q_values = tf.math.reduce_sum(self.model(states, training=True) * masks, axis=1)\n",
    "            loss = tf.keras.losses.mean_squared_error(target_q_values, estimated_q_values)\n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values)\n",
    "    \n",
    "    \n",
    "    def update_parameters(self):\n",
    "        sarts_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_1 = np.array([x[0] for x in states], dtype=\"float32\")\n",
    "        states_2 = np.array([x[1] for x in states], dtype=\"float32\")\n",
    "        \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_1 = np.array([x[0] for x in next_states], dtype=\"float32\")\n",
    "        next_states_2 = np.array([x[1] for x in next_states], dtype=\"float32\")\n",
    "        \n",
    "        masks = self.m1[actions]\n",
    "        \n",
    "        return self.tstep([states_1, states_2], [next_states_1, next_states_2], rewards, terminals, masks)\n",
    "    \n",
    "    \n",
    "    def train(self, num_steps, envs, log_interval = 1000, warmup = 0, train_steps_per_step = 1):\n",
    "        \n",
    "        num_envs = len(envs)\n",
    "        states = [x.reset(True) for x in envs]\n",
    "        \n",
    "        current_episode_reward_sum = 0\n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.longs = 0\n",
    "        self.shorts = 0\n",
    "\n",
    "        self.total_rewards = []\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        \n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            file = open(\"logs/loss_log.txt\", \"a\")  \n",
    "            for loss in self.losses:\n",
    "                        file.write(str(loss))\n",
    "                        file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(\"logs/qv_log.txt\", \"a\")  \n",
    "            for qv in self.q_v:\n",
    "                        file.write(str(qv))\n",
    "                        file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(\"logs/rewards_log.txt\", \"a\")  \n",
    "            for total_reward in self.total_rewards:\n",
    "                        file.write(str(total_reward))\n",
    "                        file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.total_rewards = []\n",
    "            self.losses = [0]\n",
    "            self.q_v = [0]\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.05, stateful_metrics = [\"reward sum\", \"t\", \"l/s\"])\n",
    "                    self.longs = 0\n",
    "                    self.shorts = 0\n",
    "\n",
    "\n",
    "                states_1 = np.array([x[0] for x in states])\n",
    "                states_2 = np.array([x[1] for x in states])\n",
    "                actions = self.select_actions(states_1, states_2)\n",
    "                for action in actions:\n",
    "                    if action == 1:\n",
    "                        self.shorts+=1\n",
    "                    elif action == 2:\n",
    "                        self.longs+=1\n",
    "\n",
    "                sasrt_pairs = []\n",
    "                for index in range(num_envs):\n",
    "                    sasrt_pairs.append([states[index], actions[index]]+[x for x in envs[index].step(actions[index])])\n",
    "\n",
    "                next_states = [x[2] for x in sasrt_pairs]\n",
    "\n",
    "                reward = [x[3] for x in sasrt_pairs]\n",
    "                current_episode_reward_sum += np.sum(reward)\n",
    "\n",
    "                self.total_rewards.extend(reward)\n",
    "\n",
    "                for index, o in enumerate(sasrt_pairs):\n",
    "                    #print(o)\n",
    "                    if o[4] == True:\n",
    "                        next_states[index] = envs[index].reset()\n",
    "                    self.observe_sasrt(o[0], o[1], o[2], o[3], o[4])\n",
    "\n",
    "                states = next_states\n",
    "                if i > warmup:\n",
    "                    for _ in range(train_steps_per_step):\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "                if (i+1) % log_interval == 0:\n",
    "                    save_current_run()\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-train_steps_per_step:])),\n",
    "                                (\"mean q\", np.mean(self.q_v[-train_steps_per_step:])),\n",
    "                                (\"rewards\", np.mean(reward)),\n",
    "                                (\"reward sum\", current_episode_reward_sum),\n",
    "                                (\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts)),\n",
    "                                (\"t\", np.mean(times))])\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c969aa-600f-446e-99b0-5064bd639bf9",
   "metadata": {},
   "source": [
    "# The Q-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6725e-d691-4cfe-ab23-12a463707bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    embed_dim = 0\n",
    "    num_heads = 0\n",
    "    ff_dim = 0 \n",
    "    rate=0\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update()\n",
    "        cfg.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return cfg  \n",
    "    def call(self, inputs, training = False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    \n",
    "def getPositionEncoding(seq_len, d, n=10000):\n",
    "    P = np.zeros((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d/2)):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P[::]\n",
    "\n",
    "class Positions(tf.keras.layers.Layer):\n",
    "    P = []\n",
    "    d = 0\n",
    "    seq_len = 0\n",
    "    def __init__(self, seq_len, d, **kwargs):\n",
    "        super(Positions, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d = d\n",
    "        self.p = getPositionEncoding(seq_len, d)\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.p\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update()\n",
    "        cfg.update({\n",
    "            'p': self.p,\n",
    "            'seq_len': self.seq_len,\n",
    "            'd': self.d\n",
    "        })\n",
    "        return cfg  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8c179-0d82-4ae1-83d1-f4f3b1261d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs_1 = tf.keras.layers.Input(shape = (seq_len, 6))\n",
    "inputs_pos = tf.keras.layers.Input(shape = (2))\n",
    "\n",
    "x = tf.keras.layers.Reshape((seq_len,6))(inputs_1)\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(32,3, padding=\"same\")(x)\n",
    "x2 = tf.keras.layers.LeakyReLU(alpha=0.1)(x2)\n",
    "x = tf.keras.layers.Concatenate()([x,x2])\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = Positions(seq_len, x.shape[-1])(x)\n",
    "x = TransformerBlock(x.shape[2], 2, 64)(x)\n",
    "x = TransformerBlock(x.shape[2], 2, 64)(x)\n",
    "\n",
    "#x = tf.keras.layers.LSTM(128, activation = \"tanh\", return_sequences = True)(x)\n",
    "x = tf.keras.layers.GRU(64, activation = \"tanh\", return_sequences = False)(x)\n",
    "#x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "#x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Concatenate()([inputs_pos, x])\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(x)\n",
    "x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "        \n",
    "outputs = tf.keras.layers.Dense(3, activation = \"linear\", use_bias=False, dtype=\"float32\")(x)\n",
    "model = tf.keras.Model([inputs_1,inputs_pos], outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4874f-1d52-4f85-8982-9b079ceaa178",
   "metadata": {},
   "source": [
    "# training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222179b4-ebaa-4daa-8ea4-7859fe68e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    model = model, \n",
    "    n_actions = 3, \n",
    "    memory_size = 1100000, \n",
    "    gamma=0.985,\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00002, clipvalue = 10.0), \n",
    "    batch_size = 256, \n",
    "    #target_model_sync = 2000, \n",
    "    name=\"deep_q_trading\")\n",
    "agent.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de3a6e-bc32-4826-a684-3defca8b445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parallel = 20\n",
    "envs = [environment() for _ in range(num_parallel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02164d2-26c8-427d-9a9d-fbfc70fe6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(30000)\n",
    "agent.train(num_steps = n, envs = envs, warmup = n, log_interval = n, train_steps_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c66e262f-dc42-4d86-a04b-2434da8ce715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81df5b-8b63-48a3-b8de-47ec9abca59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_parallel = 5\n",
    "envs = [environment() for _ in range(num_parallel)]\n",
    "\n",
    "if True:    \n",
    "    fig, ax = plt.subplots(1,num_parallel,figsize=(30,5))\n",
    "    for o in range(num_parallel):\n",
    "        x = envs[o].reset(True)[0]\n",
    "        x_ = np.transpose(x)\n",
    "        for i in x_:\n",
    "            ax[o].plot(i) if num_parallel > 1 else ax.plot(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31fd85-937a-4c9e-8fd5-92558b0c33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000000\n",
    "agent.train(num_steps = n, envs = envs, warmup = 0, log_interval = 10000, train_steps_per_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "097d3643-cd13-4f0a-b4c2-2d91cdcd0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "169c17fc-1086-4fed-8617-6c8b60f8be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = agent.memory\n",
    "#agent.memory = m\n",
    "#del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027cdeb-5da3-40cd-8a43-2676026104f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb7761-8827-4cf2-92c3-1c8f10e612ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc590c05-85cb-4afb-b92a-7f561fb279a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8aff8-ae78-4174-a9df-b5e8a218a9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a67ce3-9a02-45b2-bc52-110f89d7904f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
